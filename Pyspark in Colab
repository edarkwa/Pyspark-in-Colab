{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4_Darkwa_Emma.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spark and Spark SQL\n",
        "Emma Darkwa"
      ],
      "metadata": {
        "id": "bE-9Cxg7dA28"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kXYsSzw6aeBE"
      },
      "outputs": [],
      "source": [
        "# install Java Virtual Machine (JVM) from OpenJDK\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download and decompress Apache Spark with Hadoop from https://spark.apache.org/downloads.html\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz #dowloan is wget\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz #tar is to compress"
      ],
      "metadata": {
        "id": "aiPdf3_Saz2W"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set environment path\n",
        "import os\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
        "os.environ['SPARK_HOME'] = '/content/spark-3.2.1-bin-hadoop3.2'"
      ],
      "metadata": {
        "id": "9Hrlj1IXcTBQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install and import findspark to locate Spark on the system\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "metadata": {
        "id": "pr3AiejHcoAT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b030620b-2b2b-4375-dcc5-a847fac12e2a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.2.1-bin-hadoop3.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark resilient distributed dataset (RDD)\n",
        "#how to ask sapark to do the sotrage and how to get the data back to you\n",
        "import pyspark\n",
        "sc = pyspark.SparkContext(appName='hw4') #this is for the distribution storage\n",
        "data = list(range(7)) #this could be an actual data table\n",
        "rdd = sc.parallelize(data) #pushing the data into the hadoop storage , you can add more than 4\n",
        "rdd.getNumPartitions(), rdd.collect() #this collects the data back"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqtO5D99CN3Z",
        "outputId": "25a7fcd6-3ec4-479c-e931-743d2a3bfd49"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, [0, 1, 2, 3, 4, 5, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import SparkSession from pyspark.sql and create a SparkSession, which is the entry point to Spark\n",
        "from pyspark.sql import SparkSession #spark session is part of the SQL package\n",
        "spark = SparkSession.builder.master('local').appName('Colab').config('spark.ui.port', '4050').getOrCreate()\n",
        "spark\n",
        "#this is running the Spark configuration"
      ],
      "metadata": {
        "id": "-cfJx55Bc0Bq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "e3700573-708f-491a-950b-c3f5b9e10f99"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f2540a77610>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://f95607c405ed:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>hw4</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# authorize Colab to access Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AagtxRNgC_1G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "281149d4-71ee-4089-ae83-5f59c54f0dd5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data into PySpark\n",
        "df1 = spark.read.json('/content/drive/My Drive/hw4_customer.json')\n",
        "df2 = spark.read.json('/content/drive/My Drive/hw4_order.json')\n",
        "df3 = spark.read.json('/content/drive/My Drive/hw4_product.json')\n",
        "df4 = spark.read.json('/content/drive/My Drive/hw4_orderline.json')"
      ],
      "metadata": {
        "id": "fFMBrjYDeBaw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show column details\n",
        "df2.printSchema()"
      ],
      "metadata": {
        "id": "wRCQAeYoeHAh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ca27bb5-5d16-4d93-c875-d0969d4025e9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- customerID: long (nullable = true)\n",
            " |-- orderDate: string (nullable = true)\n",
            " |-- orderID: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# filter rows then select columns\n",
        "# 1. What are the description, finish and standard price of product, which standard price is less than $275 in the ascending order?\n",
        "c=df3.filter('productStandardPrice < 275').select('productDescription', 'productFinish', 'productStandardPrice')\n",
        "c.sort(df3.productStandardPrice.asc()).show()"
      ],
      "metadata": {
        "id": "yLWkQ8bwedX1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95a0e9eb-9ef2-43a5-cdd5-a427efcc02c8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------------+--------------------+\n",
            "| productDescription|productFinish|productStandardPrice|\n",
            "+-------------------+-------------+--------------------+\n",
            "|        48 Bookcase|       Walnut|               150.0|\n",
            "|         Nightstand|       Cherry|               150.0|\n",
            "|   Cherry End Table|       Cherry|               175.0|\n",
            "|        48 Bookcase|          Oak|               175.0|\n",
            "|Birch Coffee Tables|        Birch|               200.0|\n",
            "|        96 Bookcase|          Oak|               200.0|\n",
            "|        96 Bookcase|       Walnut|               225.0|\n",
            "|     Pine End Table|         Pine|               256.0|\n",
            "+-------------------+-------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select columns then filter rows\n",
        "#2. What are the description, finish and standard price of all desks and all tables that cost more than $300 in the descending order?\n",
        "x=df3.filter(df3.productDescription.like('%Table%') | df3.productDescription.like('%Desk%')).select('productDescription', 'productFinish', 'productStandardPrice')\n",
        "#x.show()\n",
        "n=x.filter('productStandardPrice > 300').select('productDescription', 'productFinish', 'productStandardPrice')\n",
        "n.sort('productStandardPrice', ascending=False).show()"
      ],
      "metadata": {
        "id": "_Dj7et_sYo1j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10d2487b-c8f4-4998-fad6-f42ac623b568"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-------------+--------------------+\n",
            "|productDescription|productFinish|productStandardPrice|\n",
            "+------------------+-------------+--------------------+\n",
            "| Oak Computer Desk|          Oak|               750.0|\n",
            "|     Writer's Desk|          Oak|               325.0|\n",
            "+------------------+-------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select columns then filter rows\n",
        "from pyspark.sql.functions import col\n",
        "#3. What are the description and finish of product that has been ordered in the ascending order of finish then description?\n",
        "a=df4.join(df3,df4.productID==df3.productID)\n",
        "#a.show()\n",
        "k=a.select('ProductDescription','ProductFinish').distinct()\n",
        "n=k.orderBy(a.productFinish.asc(),col(\"productDescription\").asc())\n",
        "n.show()\n"
      ],
      "metadata": {
        "id": "N7QRkOd4ehtO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e032cb52-c30b-465e-c3e7-21cad6f3c36f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------+\n",
            "|  ProductDescription|ProductFinish|\n",
            "+--------------------+-------------+\n",
            "|    8-Drawer Dresser|        Birch|\n",
            "| Birch Coffee Tables|        Birch|\n",
            "|    Cherry End Table|       Cherry|\n",
            "|Entertainment Center|       Cherry|\n",
            "|         48 Bookcase|          Oak|\n",
            "|   Oak Computer Desk|          Oak|\n",
            "|       Writer's Desk|          Oak|\n",
            "|         48 Bookcase|       Walnut|\n",
            "+--------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4.What are the name (no truncation), city and state of customer in Florida, Texas, California or Hawaii in the ascending order of name?\n",
        "#from pyspark.sql.functions import col\n",
        "li=[\"FL\",\"TX\",\"CA\",\"HI\"]\n",
        "g=df1.filter(df1.customerState.isin(li)).select('customerName', 'CustomerCity', 'CustomerState')\n",
        "g.sort(df1.customerName.asc()).show(truncate=False) "
      ],
      "metadata": {
        "id": "bqNbAtJsKOU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a305d562-6f5b-4418-9948-41a5d351d5d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------+------------+-------------+\n",
            "|customerName            |CustomerCity|CustomerState|\n",
            "+------------------------+------------+-------------+\n",
            "|California Classics     |Santa Clara |CA           |\n",
            "|Contemporary Casuals    |Gainesville |FL           |\n",
            "|Impressions             |Sacramento  |CA           |\n",
            "|Kaneohe Homes           |Kaneohe     |HI           |\n",
            "|M and H Casual Furniture|Clearwater  |FL           |\n",
            "|Seminole Interiors      |Seminole    |FL           |\n",
            "|Value Furniture         |Plano       |TX           |\n",
            "+------------------------+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from IPython.utils.decorators import flag_calls\n",
        "#5. How many customers in each of the state Florida, Texas, California or Hawaii?\n",
        "li=[\"FL\",\"TX\",\"CA\",\"HI\"]\n",
        "b=df1.filter(df1.customerState.isin(li)).select('customerID','CustomerState') \n",
        "p=b.groupBy(\"CustomerState\").count()\n",
        "ik=p.sort('count', ascending=False)\n",
        "ik.withColumnRenamed(\"count\",\"Number of customers\").show()\n"
      ],
      "metadata": {
        "id": "JPDrsT_ZK6mO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bbfad1e-8cf5-42e5-eddb-da29f32a64df"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-------------------+\n",
            "|CustomerState|Number of customers|\n",
            "+-------------+-------------------+\n",
            "|           FL|                  3|\n",
            "|           CA|                  2|\n",
            "|           TX|                  1|\n",
            "|           HI|                  1|\n",
            "+-------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6. What is the average standard price for all products in inventory?\n",
        "from pyspark.sql.functions import avg\n",
        "avgPrice=df3.select(avg(\"productStandardPrice\").alias('Average Standard Price for all products'))\n",
        "avgPrice.show()\n"
      ],
      "metadata": {
        "id": "SHp_M66jLYOK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48ecde7b-ff28-490a-aacc-66f1a1ab8391"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------+\n",
            "|Average Standard Price for all products|\n",
            "+---------------------------------------+\n",
            "|                      534.6315789473684|\n",
            "+---------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. What are the product description, product finish, and the price higher than the average standard price for all products in inventory, \n",
        "#in the descending order of price difference?\n",
        "#df_slt = df3.select('productDescription', 'productFinish','productStandardPrice')\n",
        "#avgPrice=df3.select(avg(\"productStandardPrice\")).collect()[0][0]\n",
        "#print(f'average price = {avgPrice}')\n",
        "#gh=df_slt.filter(df3.productStandardPrice > avgPrice)\n",
        "#gh.sort(gh.productStandardPrice.desc()).show()"
      ],
      "metadata": {
        "id": "zSU3rypWJCV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import lag, col\n",
        "\n",
        "#7. What are the product description, product finish, and the price higher than the average standard price for all products in inventory, \n",
        "#in the descending order of price difference?\n",
        "\n",
        "df_slt = df3.select('productDescription', 'productFinish','productStandardPrice')\n",
        "avgPrice=df3.select(avg(\"productStandardPrice\")).collect()[0][0]\n",
        "\n",
        "gv=df3.select('productDescription', 'productFinish','productStandardPrice')\n",
        "#gv.show()\n",
        "\n",
        "\n",
        "df0 = gv.withColumn(\"Price_difference\",(gv.productStandardPrice - avgPrice))\n",
        "ig=df0.sort('Price_difference', ascending=False)\n",
        "\n",
        "ol=ig.filter(df3.productStandardPrice > avgPrice).show()\n",
        "\n",
        "\n",
        "#reference https://www.arundhaj.com/blog/calculate-difference-with-previous-row-in-pyspark.html\n"
      ],
      "metadata": {
        "id": "hS2XC4shFPz4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fb732cb-04d3-4eb1-f5e9-55af3f9a8be6"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------+--------------------+------------------+\n",
            "|  productDescription|productFinish|productStandardPrice|  Price_difference|\n",
            "+--------------------+-------------+--------------------+------------------+\n",
            "|Entertainment Center|       Cherry|              1650.0|1115.3684210526317|\n",
            "|              Amoire|       Walnut|              1200.0| 665.3684210526316|\n",
            "|7' Grandfather Clock|          Oak|              1100.0| 565.3684210526316|\n",
            "|6' Grandfather Clock|          Oak|               890.0|355.36842105263156|\n",
            "|    8-Drawer Dresser|          Oak|               800.0|265.36842105263156|\n",
            "|   Oak Computer Desk|          Oak|               750.0|215.36842105263156|\n",
            "|    8-Drawer Dresser|        Birch|               750.0|215.36842105263156|\n",
            "+--------------------+-------------+--------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8.What are the order id, order date, the customer name (no truncation), and the overall total price for each order, in the ascending order of order id?\n",
        "v=df2.join(df1,df2.customerID==df1.customerID).drop(df1.customerID)\n",
        "#v.show()\n",
        "k=df4.join(df3,df4.productID==df3.productID,\"inner\").drop(df3.productID)\n",
        "#k.show()\n",
        "table=v=k.join(v,k.orderID==v.orderID, \"inner\").drop(v.orderID)\n",
        "#table.show()\n",
        "table.select('orderID', 'orderDate', 'customerName','productStandardPrice')\n",
        "table.groupBy('orderID','orderDate', 'customerName').agg(F.sum('productStandardPrice').alias(\"Overall total price for each order($)\")).sort(table.orderID).show(truncate=False)\n",
        "\n",
        "\n",
        "#reference https://stackoverflow.com/questions/64030299/add-total-per-group-as-a-new-row-in-dataframe-in-pyspark\n"
      ],
      "metadata": {
        "id": "6yZctQ2xYtb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4baf452-3ffa-4217-b285-4c6c91b73c8d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------------------------+-------------------------------------+\n",
            "|orderID|orderDate |customerName            |Overall total price for each order($)|\n",
            "+-------+----------+------------------------+-------------------------------------+\n",
            "|1001   |2010-10-21|Contemporary Casuals    |2025.0                               |\n",
            "|1002   |2010-10-21|California Classics     |750.0                                |\n",
            "|1003   |2010-10-22|Mountain Scenes         |750.0                                |\n",
            "|1004   |2010-10-22|Impressions             |925.0                                |\n",
            "|1005   |2010-10-24|Home Furnishings        |1650.0                               |\n",
            "|1006   |2010-10-24|Value Furniture         |2125.0                               |\n",
            "|1007   |2010-10-27|American Euro Lifestyles|375.0                                |\n",
            "|1008   |2010-10-30|Battle Creek Furniture  |925.0                                |\n",
            "|1009   |2010-11-05|Eastern Furniture       |1800.0                               |\n",
            "|1010   |2010-11-05|Contemporary Casuals    |175.0                                |\n",
            "+-------+----------+------------------------+-------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8.What are the order id, order date, the customer name (no truncation), and the overall total price for each order, in the ascending order of order id?\n",
        "v=df2.join(df1,df2.customerID==df1.customerID).drop(df1.customerID)\n",
        "#v.show()\n",
        "k=df4.join(df3,df4.productID==df3.productID,\"inner\").drop(df3.productID)\n",
        "#k.show()\n",
        "table=v=k.join(v,k.orderID==v.orderID, \"inner\").drop(v.orderID)\n",
        "#table.show()\n",
        "#order_item_subtotal =  (table.productStandardPrice * table.orderedQuantity)\n",
        "\n",
        "table.select('orderID', 'orderDate', 'customerName','productStandardPrice')\n",
        "#table.productStandardPrice * table.orderedQuantity\n",
        "\n",
        "table.groupBy('orderID','orderDate', 'customerName').agg(F.sum(table.productStandardPrice * table.orderedQuantity).alias(\"Overall total price for each order($)\")).sort(table.orderID).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiIS3tm5oQU1",
        "outputId": "60dff47f-2915-46ff-f4cd-0d4671760eef"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------------------------+-------------------------------------+\n",
            "|orderID|orderDate |customerName            |Overall total price for each order($)|\n",
            "+-------+----------+------------------------+-------------------------------------+\n",
            "|1001   |2010-10-21|Contemporary Casuals    |2400.0                               |\n",
            "|1002   |2010-10-21|California Classics     |3750.0                               |\n",
            "|1003   |2010-10-22|Mountain Scenes         |2250.0                               |\n",
            "|1004   |2010-10-22|Impressions             |1850.0                               |\n",
            "|1005   |2010-10-24|Home Furnishings        |4950.0                               |\n",
            "|1006   |2010-10-24|Value Furniture         |2600.0                               |\n",
            "|1007   |2010-10-27|American Euro Lifestyles|925.0                                |\n",
            "|1008   |2010-10-30|Battle Creek Furniture  |2775.0                               |\n",
            "|1009   |2010-11-05|Eastern Furniture       |3750.0                               |\n",
            "|1010   |2010-11-05|Contemporary Casuals    |1750.0                               |\n",
            "+-------+----------+------------------------+-------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9. What are the id, name (no truncation), full address, and number of orders (0 if no order) for all customers, in the ascending order of customer id?\n",
        "\n",
        "#pl=df1.join(df2,df1.customerID==df2.customerID,\"outer\").drop(df2.customerID)\n",
        "#pl.show()\n",
        "#jk=pl.join(df4,pl.orderID==df4.orderID,\"outer\").drop(df4.orderID)\n",
        "#jk.show()\n",
        "\n",
        "\n",
        "#nb=jk.select('customerID', 'customerName',(concat(jk.customerAddress,jk.customerCity, jk.customerState, jk.customerPostalCode).alias(\"Customer Full Address\")),'orderID')\n",
        "#nb.select('customerID', 'customerName','Customer Full Address','orderID')\n",
        "#nb.groupBy('customerID','customerName','Customer Full Address').agg(F.count('orderID').alias(\"Number of orders\")).sort(nb.customerID).show(truncate=False)\n",
        "\n",
        "#table.groupBy('customerID','customerName','Customer Full Address')\n",
        "#ok.withColumnRenamed(\"count\",\"Number of orders\").sort('customerID', ascending=True).show(truncate=False)\n",
        "\n",
        "#table.select('orderID', 'orderDate', 'customerName','productStandardPrice')\n",
        "#table.groupBy('orderID','orderDate', 'customerName').agg(F.sum('productStandardPrice').alias(\"Overall total price for each order($)\")).sort(table.orderID).show(truncate=False)\n"
      ],
      "metadata": {
        "id": "s7EuDYSAza6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9. What are the id, name (no truncation), full address, and number of orders (0 if no order) for all customers, in the ascending order of customer id?\n",
        "from pyspark.sql.functions import concat,lit,col\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "\n",
        "#df4.show()\n",
        "pl=df1.join(df2,df1.customerID==df2.customerID,\"outer\").drop(df2.customerID)\n",
        "#pl.show()\n",
        "jk=pl.join(df4,pl.orderID==df4.orderID,\"outer\").drop(pl.orderID)\n",
        "#jk.show()\n",
        "\n",
        "\n",
        "nb=jk.select('customerID', 'customerName',(concat(jk.customerAddress,jk.customerCity, jk.customerState, jk.customerPostalCode).alias(\"Customer Full Address\")),'orderID')\n",
        "nb.select('customerID', 'customerName','Customer Full Address','orderID')\n",
        "g=nb.groupBy('customerID','customerName','Customer Full Address').agg(F.count('orderID').alias(\"Number of orders\")).sort(nb.customerID)\n",
        "kl=g.withColumnRenamed(\"count\",\"Number of orders\").na.fill(value=0).sort('customerID', ascending=True).show(truncate=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LYcHQi2biFx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b36e262e-b137-4d46-a5f2-c7b892cf8f7c"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------------+---------------------------------------------+----------------+\n",
            "|customerID|customerName            |Customer Full Address                        |Number of orders|\n",
            "+----------+------------------------+---------------------------------------------+----------------+\n",
            "|1         |Contemporary Casuals    |1355 S Hines BlvdGainesvilleFL32601-2871     |4               |\n",
            "|2         |Value Furniture         |15145 S.W. 17th St.PlanoTX75094-7743         |3               |\n",
            "|3         |Home Furnishings        |1900 Allard Ave.AlbanyNY12209-1125           |1               |\n",
            "|4         |Eastern Furniture       |1925 Beltline Rd.CarteretNJ07008-3188        |2               |\n",
            "|5         |Impressions             |5585 Westcott Ct.SacramentoCA94206-4056      |2               |\n",
            "|6         |Furniture Gallery       |325 Flatiron Dr.BoulderCO80514-4432          |0               |\n",
            "|7         |Period Furniture        |394 Rainbow Dr.SeattleWA97954-5589           |0               |\n",
            "|8         |California Classics     |816 Peach Rd.Santa ClaraCA96915-7754         |1               |\n",
            "|9         |M and H Casual Furniture|3709 First StreetClearwaterFL34620-2314      |0               |\n",
            "|10        |Seminole Interiors      |2400 Rocky Point Dr.SeminoleFL34646-4423     |0               |\n",
            "|11        |American Euro Lifestyles|2424 Missouri Ave N.Prospect ParkNJ07508-5621|2               |\n",
            "|12        |Battle Creek Furniture  |345 Capitol Ave. SWBattle CreekMI49015-3401  |2               |\n",
            "|13        |Heritage Furnishings    |66789 College Ave.CarlislePA17013-8834       |0               |\n",
            "|14        |Kaneohe Homes           |112 Kiowai St.KaneoheHI96744-2537            |0               |\n",
            "|15        |Mountain Scenes         |4132 Main StreetOgdenUT84403-4432            |1               |\n",
            "+----------+------------------------+---------------------------------------------+----------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}